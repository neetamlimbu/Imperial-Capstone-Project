{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-25T02:52:54.763569600Z",
     "start_time": "2025-12-25T02:52:51.903812700Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import KFold\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import warnings\n",
    "\n",
    "# Optional: quiet down convergence warnings for tiny datasets\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 1: Week 1–3 inputs & outputs\n",
    "# -----------------------------\n",
    "week_inputs = {\n",
    "    1: [[0.333333, 0.666667], [0.5, 0.5], [0.45, 0.55]],\n",
    "    2: [[0.777778, 0.222222], [0.7, 0.3], [0.725, 0.275]],\n",
    "    3: [[0.142857, 0.571429, 0.857143], [0.2, 0.6, 0.8], [0.8, 0.2, 0.4]],\n",
    "    4: [[0.285714, 0.714286, 0.428571, 0.857143], [0.2, 0.8, 0.3, 0.7], [0.25, 0.75, 0.35, 0.65]],\n",
    "    5: [[0.0625, 0.5, 0.9375, 0.25], [0.08, 0.52, 0.92, 0.27], [0.07, 0.51, 0.93, 0.26]],\n",
    "    6: [[0.111111, 0.444444, 0.777778, 0.222222, 0.888889], [0.2, 0.5, 0.8, 0.3, 0.9], [0.21, 0.49, 0.81, 0.31, 0.91]],\n",
    "    7: [[0.090909, 0.363636, 0.636364, 0.181818, 0.545455, 0.818182], [0.12, 0.38, 0.66, 0.22, 0.58, 0.84], [0.1, 0.36, 0.64, 0.2, 0.56, 0.82]],\n",
    "    8: [[0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 0.0625], [0.15, 0.275, 0.4, 0.525, 0.65, 0.775, 0.9, 0.1], [0.13, 0.26, 0.38, 0.51, 0.63, 0.76, 0.88, 0.07]]\n",
    "}\n",
    "\n",
    "week_outputs = {\n",
    "    1: [5.72e-48, 2.67e-09, 1.55e-13],\n",
    "    2: [0.1668, 0.4380, 0.4116],\n",
    "    3: [-0.0351, -0.0651, -0.0390],\n",
    "    4: [-16.18, -15.30, -11.86],\n",
    "    5: [94.62, 73.85, 85.55],\n",
    "    6: [-1.77, -1.72, -1.82],\n",
    "    7: [1.06, 0.84, 1.01],\n",
    "    8: [8.67, 8.53, 8.63]\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Step 2: Generate Week 4 inputs (avg of Weeks 1–3)\n",
    "# -----------------------------\n",
    "def generate_week4_inputs(inputs):\n",
    "    return np.mean(np.array(inputs), axis=0)\n",
    "\n",
    "week4_inputs = {i: generate_week4_inputs(week_inputs[i]) for i in week_inputs}\n",
    "\n",
    "# -----------------------------\n",
    "# Step 3: Train & evaluate with hybrid scoring\n",
    "# -----------------------------\n",
    "models = {\n",
    "    \"Gradient Boosting\": GradientBoostingRegressor(),\n",
    "    \"Random Forest\": RandomForestRegressor(),\n",
    "    \"SVR\": SVR(),\n",
    "    \"Neural Network\": MLPRegressor(max_iter=1000, hidden_layer_sizes=(20,), random_state=42)\n",
    "}\n",
    "\n",
    "results = []\n",
    "per_model_metrics_rows = []\n",
    "\n",
    "for idx in week_inputs:\n",
    "    X_train = np.array([np.pad(x, (0, 8 - len(x)), 'constant') for x in week_inputs[idx]])\n",
    "    y_train = np.array(week_outputs[idx])\n",
    "    X_test = np.pad(week4_inputs[idx], (0, 8 - len(week4_inputs[idx])), 'constant').reshape(1, -1)\n",
    "\n",
    "    # Collect metrics for normalization per index\n",
    "    model_metric_cache = []\n",
    "\n",
    "    for name, model in models.items():\n",
    "        kf = KFold(n_splits=2, shuffle=False)\n",
    "        mse_scores, mae_scores = [], []\n",
    "\n",
    "        for train_idx, val_idx in kf.split(X_train):\n",
    "            model.fit(X_train[train_idx], y_train[train_idx])\n",
    "            preds = model.predict(X_train[val_idx])\n",
    "            mse_scores.append(mean_squared_error(y_train[val_idx], preds))\n",
    "            mae_scores.append(mean_absolute_error(y_train[val_idx], preds))\n",
    "\n",
    "        avg_mse, avg_mae = float(np.mean(mse_scores)), float(np.mean(mae_scores))\n",
    "\n",
    "        # Final fit and Week 4 prediction\n",
    "        model.fit(X_train, y_train)\n",
    "        pred = float(model.predict(X_test)[0])\n",
    "\n",
    "        # Stability penalty: deviation from last week\n",
    "        stability_penalty = abs(pred - y_train[-1])\n",
    "\n",
    "        model_metric_cache.append({\n",
    "            \"Index\": idx,\n",
    "            \"Model\": name,\n",
    "            \"CV_MSE\": avg_mse,\n",
    "            \"CV_MAE\": avg_mae,\n",
    "            \"StabilityPenalty\": float(stability_penalty),\n",
    "            \"PredictedOutput\": pred\n",
    "        })\n",
    "\n",
    "    # Normalize metrics within the index (min-max) to combine fairly\n",
    "    df_metrics = pd.DataFrame(model_metric_cache)\n",
    "    # Avoid zero division by adding small epsilon\n",
    "    eps = 1e-12\n",
    "    for col in [\"CV_MSE\", \"CV_MAE\", \"StabilityPenalty\"]:\n",
    "        cmin, cmax = df_metrics[col].min(), df_metrics[col].max()\n",
    "        if cmax - cmin < eps:\n",
    "            df_metrics[col + \"_N\"] = 0.0  # all equal; no preference\n",
    "        else:\n",
    "            df_metrics[col + \"_N\"] = (df_metrics[col] - cmin) / (cmax - cmin)\n",
    "\n",
    "    # Hybrid score: 0.5*MSE + 0.3*MAE + 0.2*Stability\n",
    "    df_metrics[\"HybridScore\"] = (\n",
    "        0.5 * df_metrics[\"CV_MSE_N\"] +\n",
    "        0.3 * df_metrics[\"CV_MAE_N\"] +\n",
    "        0.2 * df_metrics[\"StabilityPenalty_N\"]\n",
    "    )\n",
    "\n",
    "    # Select best model\n",
    "    best_row = df_metrics.loc[df_metrics[\"HybridScore\"].idxmin()]\n",
    "\n",
    "    prev = y_train[-1]\n",
    "    best_pred = float(best_row[\"PredictedOutput\"])\n",
    "    gain = ((best_pred - prev) / abs(prev)) * 100 if prev != 0 else 0.0\n",
    "\n",
    "    results.append({\n",
    "        \"Index\": idx,\n",
    "        \"Week4 Inputs\": week4_inputs[idx],\n",
    "        \"Predicted Output\": best_pred,\n",
    "        \"Selected Model\": best_row[\"Model\"],\n",
    "        \"Hybrid Score\": float(best_row[\"HybridScore\"]),\n",
    "        \"Percentage Gain vs Week3\": float(gain),\n",
    "        \"Reason\": f\"Lowest hybrid score ({best_row['HybridScore']:.4f}) among models\"\n",
    "    })\n",
    "\n",
    "    # Save per-model metrics for Excel\n",
    "    per_model_metrics_rows.extend(df_metrics.to_dict(\"records\"))\n",
    "\n",
    "# -----------------------------\n",
    "# Step 4: Print results\n",
    "# -----------------------------\n",
    "df_results = pd.DataFrame(results)\n",
    "print(\"\\n=== Week 4 Model Selection Results (Hybrid Scoring) ===\\n\")\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# -----------------------------\n",
    "# Step 5: Save to Excel (includes per-model metrics)\n",
    "# -----------------------------\n",
    "summary_text = \"\"\"\n",
    "Hybrid Scoring Findings:\n",
    "- Gradient Boosting strong for nonlinear recovery.\n",
    "- Random Forest preferred for high-variance outputs.\n",
    "- SVR chosen for oscillations when stable.\n",
    "- Neural Network selected for subtle nonlinear corrections on some indices.\n",
    "\n",
    "Hybrid scoring balances CV MSE, CV MAE, and prediction stability to avoid picking models\n",
    "that have low error on tiny CV folds but produce unstable week-to-week predictions.\n",
    "\"\"\"\n",
    "\n",
    "df_summary = pd.DataFrame({\"Executive Summary\": [summary_text]})\n",
    "df_week13 = pd.DataFrame(week_outputs)\n",
    "df_per_model = pd.DataFrame(per_model_metrics_rows)\n",
    "\n",
    "with pd.ExcelWriter(\"week4_full.xlsx\") as writer:\n",
    "    df_week13.to_excel(writer, sheet_name=\"Week1-3 Comparison\", index=False)\n",
    "    df_results.to_excel(writer, sheet_name=\"Week4 Predictions\", index=False)\n",
    "    df_per_model.to_excel(writer, sheet_name=\"Per-Model Metrics\", index=False)\n",
    "    df_summary.to_excel(writer, sheet_name=\"Executive Summary\", index=False)\n",
    "\n",
    "# -----------------------------\n",
    "# Step 6: Generate charts & PDF\n",
    "# -----------------------------\n",
    "# Trend chart (Weeks 1–3)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for idx in week_outputs:\n",
    "    plt.plot([1, 2, 3], week_outputs[idx], marker='o', label=f'Index {idx}')\n",
    "plt.title(\"Week 1–3 Outputs Trend\")\n",
    "plt.xlabel(\"Week\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.legend(ncol=2)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"trend_chart.png\")\n",
    "plt.close()\n",
    "\n",
    "# Gains chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "gains = [r[\"Percentage Gain vs Week3\"] for r in results]\n",
    "plt.bar(range(1, 9), gains, color=\"#4C78A8\")\n",
    "plt.title(\"Percentage Gain vs Week 3\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Gain (%)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"gain_chart.png\")\n",
    "plt.close()\n",
    "\n",
    "# Model selection chart\n",
    "plt.figure(figsize=(8, 5))\n",
    "models_selected = [r[\"Selected Model\"] for r in results]\n",
    "model_counts = pd.Series(models_selected).value_counts()\n",
    "plt.bar(model_counts.index, model_counts.values, color=\"#F58518\")\n",
    "plt.title(\"Model Selections (Hybrid Scoring)\")\n",
    "plt.xlabel(\"Model\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_selection.png\")\n",
    "plt.close()\n",
    "\n",
    "# PDF assembly\n",
    "with PdfPages(\"week4_report.pdf\") as pdf:\n",
    "    # Executive summary page\n",
    "    plt.figure(figsize=(8.5, 11))\n",
    "    plt.text(0.1, 0.92, \"Executive Summary\", fontsize=18, weight='bold')\n",
    "    plt.text(0.1, 0.86, summary_text, fontsize=11)\n",
    "    plt.axis(\"off\")\n",
    "    pdf.savefig()\n",
    "    plt.close()\n",
    "\n",
    "    # Charts pages\n",
    "    for chart_file in [\"trend_chart.png\", \"gain_chart.png\", \"model_selection.png\"]:\n",
    "        img = plt.imread(chart_file)\n",
    "        plt.figure(figsize=(11, 8.5))\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "\n",
    "print(\"✅ week4_full.xlsx and week4_report.pdf created successfully\")"
   ],
   "id": "108e22d8030ab67",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Week 4 Model Selection Results (Hybrid Scoring) ===\n",
      "\n",
      " Index                                                                                                                Week4 Inputs  Predicted Output    Selected Model  Hybrid Score  Percentage Gain vs Week3                                    Reason\n",
      "     1                                                                                   [0.42777766666666667, 0.5722223333333333]      8.188537e-10     Random Forest  0.000000e+00             528192.731183 Lowest hybrid score (0.0000) among models\n",
      "     2                                                                                    [0.7342593333333333, 0.2657406666666667]      4.115981e-01 Gradient Boosting  0.000000e+00                 -0.000470 Lowest hybrid score (0.0000) among models\n",
      "     3                                                               [0.38095233333333334, 0.4571429999999999, 0.6857143333333333]     -3.510030e-02 Gradient Boosting  3.936153e-02                  9.999230 Lowest hybrid score (0.0394) among models\n",
      "     4                                                    [0.24523799999999998, 0.754762, 0.35952366666666663, 0.7357143333333332]     -1.432747e+01    Neural Network  6.375865e-02                -20.804978 Lowest hybrid score (0.0638) among models\n",
      "     5                                                                       [0.07083333333333335, 0.51, 0.9291666666666667, 0.26]      8.554993e+01               SVR  2.271570e-07                 -0.000085 Lowest hybrid score (0.0000) among models\n",
      "     6                                          [0.17370366666666667, 0.478148, 0.795926, 0.27740733333333334, 0.8996296666666668]     -1.770000e+00               SVR  1.839422e-01                  2.747253 Lowest hybrid score (0.1839) among models\n",
      "     7 [0.10363633333333333, 0.36787866666666663, 0.6454546666666667, 0.20060599999999998, 0.5618183333333334, 0.8260606666666667]      1.032100e+00     Random Forest  7.104648e-02                  2.188119 Lowest hybrid score (0.0710) among models\n",
      "     8 [0.135, 0.26166666666666666, 0.385, 0.5116666666666666, 0.6349999999999999, 0.7616666666666667, 0.8849999999999999, 0.0775]      8.637400e+00     Random Forest  1.862707e-02                  0.085747 Lowest hybrid score (0.0186) among models\n",
      "✅ week4_full.xlsx and week4_report.pdf created successfully\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "90825279e4cd1d7e"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
